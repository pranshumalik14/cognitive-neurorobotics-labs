@article{smyth1987,
title = {Functions of vision in the control of handwriting},
journal = {Acta Psychologica},
volume = {65},
number = {1},
pages = {47-64},
year = {1987},
issn = {0001-6918},
doi = {https://doi.org/10.1016/0001-6918(87)90046-1},
url = {https://www.sciencedirect.com/science/article/pii/0001691887900461},
author = {Mary M. Smyth and Gil Silvers},
abstract = {Much recent work on handwriting has emphasized the organisational processes which occur before output and has minimised the role of feedback processes, although reports of dysgraphic patients suggest that such processes have an important role in control. In the experiment reported here subjects wrote a set of sentences in each of six conditions, three with sight of the hand, and three without. In each visual condition subjects wrote normally, while counting aloud, and while repeating a nonsense syllable aloud. The orientation of the written words to the horizontal was affected by the loss of vision rather than by the addition of a secondary task, but the errors produced during writing were very similar for the secondary task condition and the without-vision condition. Errors occur at several levels in the output system which suggests that the role of sensory feedback is a complex one. Editing or monitoring functions can be compared with place keeping or sequencing control functions but current models are not well enough specified to help us discriminate between these.}
}

@article{teasdale1993,
title = {The role of proprioceptive information for the production of isometric forces and for handwriting tasks},
journal = {Acta Psychologica},
volume = {82},
number = {1},
pages = {179-191},
year = {1993},
issn = {0001-6918},
doi = {https://doi.org/10.1016/0001-6918(93)90011-F},
url = {https://www.sciencedirect.com/science/article/pii/000169189390011F},
author = {N. Teasdale and R. Forget and C. Bard and J. Paillard and M. Fleury and Y. Lamarre},
abstract = {A patient showing a total loss of all the large sensory myelinated fibers but intact peripheral motor system produced simple isometric force pulses and more complex tasks like handwriting and drawing. Overall, the patient was able to perform the isometric force task with an accuracy that approached that of normal subjects. The writing tasks, however, proved to be more challenging. In absence of vision, the different forms and cursive trajectories forming letters (morphocinetic components) were preserved but their localization within the constraints of the graphic space (topocinetic components) were severely impaired. These results demonstrate that, in absence of visual information, proprioceptive information is necessary to calibrate the hand in space.}
}

@article {dpca,
article_type = {journal},
title = {Demixed principal component analysis of neural population data},
author = {Kobak, Dmitry and Brendel, Wieland and Constantinidis, Christos and Feierstein, Claudia E and Kepecs, Adam and Mainen, Zachary F and Qi, Xue-Lian and Romo, Ranulfo and Uchida, Naoshige and Machens, Christian K},
editor = {van Rossum, Mark CW},
volume = 5,
year = 2016,
month = {apr},
pub_date = {2016-04-12},
pages = {e10989},
citation = {eLife 2016;5:e10989},
doi = {10.7554/eLife.10989},
url = {https://doi.org/10.7554/eLife.10989},
abstract = {Neurons in higher cortical areas, such as the prefrontal cortex, are often tuned to a variety of sensory and motor variables, and are therefore said to display mixed selectivity. This complexity of single neuron responses can obscure what information these areas represent and how it is represented. Here we demonstrate the advantages of a new dimensionality reduction technique, demixed principal component analysis (dPCA), that decomposes population activity into a few components. In addition to systematically capturing the majority of the variance of the data, dPCA also exposes the dependence of the neural representation on task parameters such as stimuli, decisions, or rewards. To illustrate our method we reanalyze population data from four datasets comprising different species, different cortical areas and different experimental tasks. In each case, dPCA provides a concise way of visualizing the data that summarizes the task-dependent features of the population response in a single figure.},
keywords = {prefrontal cortex, principal component analysis, dimensionality reduction, population activity},
journal = {eLife},
issn = {2050-084X},
publisher = {eLife Sciences Publications, Ltd},
}

@article {motornet,
article_type = {journal},
title = {MotorNet, a Python toolbox for controlling differentiable biomechanical effectors with artificial neural networks},
author = {Codol, Olivier and Michaels, Jonathan A and Kashefi, Mehrdad and Pruszynski, J Andrew and Gribble, Paul L},
editor = {Gallego, Juan Alvaro and Makin, Tamar R},
volume = 12,
year = 2024,
month = {jul},
pub_date = {2024-07-30},
pages = {RP88591},
citation = {eLife 2024;12:RP88591},
doi = {10.7554/eLife.88591},
url = {https://doi.org/10.7554/eLife.88591},
abstract = {Artificial neural networks (ANNs) are a powerful class of computational models for unravelling neural mechanisms of brain function. However, for neural control of movement, they currently must be integrated with software simulating biomechanical effectors, leading to limiting impracticalities: (1) researchers must rely on two different platforms and (2) biomechanical effectors are not generally differentiable, constraining researchers to reinforcement learning algorithms despite the existence and potential biological relevance of faster training methods. To address these limitations, we developed MotorNet, an open-source Python toolbox for creating arbitrarily complex, differentiable, and biomechanically realistic effectors that can be trained on user-defined motor tasks using ANNs. MotorNet is designed to meet several goals: ease of installation, ease of use, a high-level user-friendly application programming interface, and a modular architecture to allow for flexibility in model building. MotorNet requires no dependencies outside Python, making it easy to get started with. For instance, it allows training ANNs on typically used motor control models such as a two joint, six muscle, planar arm within minutes on a typical desktop computer. MotorNet is built on PyTorch and therefore can implement any network architecture that is possible using the PyTorch framework. Consequently, it will immediately benefit from advances in artificial intelligence through PyTorch updates. Finally, it is open source, enabling users to create and share their own improvements, such as new effector and network architectures or custom task designs. MotorNet's focus on higher-order model and task design will alleviate overhead cost to initiate computational projects for new researchers by providing a standalone, ready-to-go framework, and speed up efforts of established computational teams by enabling a focus on concepts and ideas over implementation.},
keywords = {motor control, motor learning, computational model, neural network, muscle model, biomechanical model},
journal = {eLife},
issn = {2050-084X},
publisher = {eLife Sciences Publications, Ltd},
}

@article {Sussillo2013-nc,
  title     = "Opening the black box: low-dimensional dynamics in
               high-dimensional recurrent neural networks",
  author    = "Sussillo, David and Barak, Omri",
  journal   = "Neural Comput.",
  publisher = "MIT Press - Journals",
  volume    =  25,
  number    =  3,
  pages     = "626--649",
  abstract  = "Recurrent neural networks (RNNs) are useful tools for learning
               nonlinear relationships between time-varying inputs and outputs
               with complex temporal dependencies. Recently developed algorithms
               have been successful at training RNNs to perform a wide variety
               of tasks, but the resulting networks have been treated as black
               boxes: their mechanism of operation remains unknown. Here we
               explore the hypothesis that fixed points, both stable and
               unstable, and the linearized dynamics around them, can reveal
               crucial aspects of how RNNs implement their computations.
               Further, we explore the utility of linearization in areas of
               phase space that are not true fixed points but merely points of
               very slow movement. We present a simple optimization technique
               that is applied to trained RNNs to find the fixed and slow points
               of their dynamics. Linearization around these slow regions can be
               used to explore, or reverse-engineer, the behavior of the RNN. We
               describe the technique, illustrate it using simple examples, and
               finally showcase it on three high-dimensional RNN examples: a
               3-bit flip-flop device, an input-dependent sine wave generator,
               and a two-point moving average. In all cases, the mechanisms of
               trained networks could be inferred from the sets of fixed and
               slow points and the linearized dynamics around them.",
  month     =  mar,
  year      =  2013,
  language  = "en"
}

@article {Radhakrishnan2020-pnas,
  title     = "Overparameterized neural networks implement associative memory",
  author    = "Radhakrishnan, Adityanarayanan and Belkin, Mikhail and Uhler,
               Caroline",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "Proceedings of the National Academy of Sciences",
  volume    =  117,
  number    =  44,
  pages     = "27162--27170",
  abstract  = "Identifying computational mechanisms for memorization and
               retrieval of data is a long-standing problem at the intersection
               of machine learning and neuroscience. Our main finding is that
               standard overparameterized deep neural networks trained using
               standard optimization methods implement such a mechanism for
               real-valued data. We provide empirical evidence that 1)
               overparameterized autoencoders store training samples as
               attractors and thus iterating the learned map leads to sample
               recovery, and that 2) the same mechanism allows for encoding
               sequences of examples and serves as an even more efficient
               mechanism for memory than autoencoding. Theoretically, we prove
               that when trained on a single example, autoencoders store the
               example as an attractor. Lastly, by treating a sequence encoder
               as a composition of maps, we prove that sequence encoding
               provides a more efficient mechanism for memory than autoencoding.",
  month     =  nov,
  year      =  2020,
  keywords  = "associative memory; autoencoders; neural networks;
               overparameterization; sequence encoders",
  language  = "en"
}
